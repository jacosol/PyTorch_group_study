{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLAS and LAPACK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"BLAS is a collection of low-level matrix and vector arithmetic operations (“multiply a vector by a scalar”, “multiply two matrices and add to a third matrix”, etc ...).\n",
    "\n",
    "LAPACK is a collection of higher-level linear algebra operations. Things like matrix factorizations (LU, LLt, QR, SVD, Schur, etc) that are used to do things like “find the eigenvalues of a matrix”, or “find the singular values of a matrix”, or “solve a linear system”. LAPACK is built on top of the BLAS; many users of LAPACK only use the LAPACK interfaces and never need to be aware of the BLAS at all. LAPACK is generally compiled separately from the BLAS, and can use whatever highly-optimized BLAS implementation you have available.\"\n",
    "\n",
    "from SO answer here https://stackoverflow.com/questions/17858104/what-is-the-relation-between-blas-lapack-and-atlas\\\n",
    "\n",
    "docs from the intel website https://software.intel.com/en-us/mkl-developer-reference-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix and vector algebra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm : tensor([[10., 10., 10.],\n",
      "        [10., 10., 10.]])\n",
      "bmm : tensor([[[10., 10., 10.],\n",
      "         [10., 10., 10.]],\n",
      "\n",
      "        [[10., 10., 10.],\n",
      "         [10., 10., 10.]],\n",
      "\n",
      "        [[10., 10., 10.],\n",
      "         [10., 10., 10.]],\n",
      "\n",
      "        [[10., 10., 10.],\n",
      "         [10., 10., 10.]]])\n",
      "addbmm : tensor([[90., 90., 90.],\n",
      "        [90., 90., 90.]])\n"
     ]
    }
   ],
   "source": [
    "# regular matrix multiplication\n",
    "mat1 = torch.ones(2,10)\n",
    "mat2 = torch.ones(10,3)\n",
    "res = torch.mm(mat1, mat2)\n",
    "print(f'mm : {res}')\n",
    "\n",
    "# multipling a batch of matrices\n",
    "M = torch.ones(2,3)*100\n",
    "bmat1 = torch.ones(4,2,10)\n",
    "bmat2 = torch.ones(4,10,3)\n",
    "res = torch.bmm(bmat1, bmat2)\n",
    "print(f'bmm : {res}')\n",
    "\n",
    "# multipling and adding together a batch of matrices then add a matrix to the end result\n",
    "M = torch.ones(2,3)*100\n",
    "bmat1 = torch.ones(4,2,10)\n",
    "bmat2 = torch.ones(4,10,3)\n",
    "res = torch.addbmm(M, bmat1, bmat2, alpha=1, beta=0.5)\n",
    "print(f'addbmm : {res}')\n",
    "\n",
    "# other variants are present like: torch.addmm, torch.baddbmm,  .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv : tensor([ 0.5911, -1.8514])\n",
      "addmv : tensor([8.6583, 8.2520])\n"
     ]
    }
   ],
   "source": [
    "# multiply a matrix and a vector\n",
    "mat = torch.randn(2, 3)\n",
    "vec = torch.ones(3)\n",
    "res = torch.mv(mat, vec)\n",
    "print(f'mv : {res}')\n",
    "\n",
    "# multiply a matrix and a vector and add another vector\n",
    "V = torch.ones(2)*10\n",
    "mat = torch.randn(2, 3)\n",
    "vec = torch.ones(3)\n",
    "res = torch.addmv(V, mat, vec)\n",
    "print(f'addmv : {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix*matrix :tensor([[-0.2968, -2.0897],\n",
      "        [ 1.0259,  0.2704]])\n",
      "\n",
      "vector*matrix :tensor([1.2198, 0.2322])\n",
      "\n",
      "matrix*vector :tensor([3.9841, 0.6078, 1.2510])\n",
      "\n",
      "vector*vector :-0.6373900771141052\n",
      "\n",
      "3Dtensor*vector :tensor([[-0.2895, -0.6748],\n",
      "        [-0.6835,  0.6867],\n",
      "        [ 0.3335,  0.9583],\n",
      "        [ 0.1476, -0.2026],\n",
      "        [-0.2862,  0.3676]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# matmul: multiplying tensors with any dimension (generalization)\n",
    "\"\"\"\n",
    "The behavior depends on the dimensionality of the tensors as follows:\n",
    "\n",
    "If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "If the first argument is 1-dimensional and the second argument is 2-dimensional, \n",
    "    a 1 is prepended to its dimension for the purpose of the matrix multiply. \n",
    "    After the matrix multiply, the prepended dimension is removed.\n",
    "If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
    "    the matrix-vector product is returned.\n",
    "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), \n",
    "    then a batched matrix multiply is returned. If the first argument is 1-dimensional, \n",
    "    a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. \n",
    "    If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if tensor1 is a (j \\times 1 \\times n \\times m)(j×1×n×m) tensor and tensor2 is a (k \\times m \\times p)(k×m×p) tensor, out will be an (j \\times k \\times n \\times p)(j×k×n×p) tensor.\n",
    "\"\"\"\n",
    "tensor1 = torch.randn(2,3)\n",
    "tensor2 = torch.randn(3,2)\n",
    "print(f'matrix*matrix :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3,2)\n",
    "print(f'vector*matrix :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(3,2)\n",
    "tensor2 = torch.randn(2)\n",
    "print(f'matrix*vector :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "print(f'vector*vector :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(5,2,3)\n",
    "tensor2 = torch.randn(3)\n",
    "print(f'3Dtensor*vector :{torch.matmul(tensor1, tensor2)}', end='\\n\\n') # also the other way around works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7495,  0.6446],\n",
      "        [-0.5327,  1.9050]])\n"
     ]
    }
   ],
   "source": [
    "# chain matrix multuplication\n",
    "mat1 = torch.randn(2,3)\n",
    "mat2 = torch.randn(3,4)\n",
    "mat3 = torch.randn(4,2)\n",
    "res = torch.chain_matmul(mat1, mat2, mat3)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outer product\n",
    "torch.ger(torch.Tensor([1,2]),torch.Tensor([3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other obvious function are present like: torch.matrix_power(input, n), torch.dot(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting matrix attributes (det, rank, eigenvalues..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.inverse\n",
    "torch.det \n",
    "torch.eig # eigenvalues and eigenvectors\n",
    "torch.matrix_rank\n",
    "# .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix decompositions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = tensor([[-0.9239, -0.3827],\n",
      "        [-0.3827,  0.9239]]) \n",
      " S = tensor([1.8478, 0.7654]) \n",
      " V = tensor([[-0.5000, -0.5000],\n",
      "        [-0.7071,  0.7071],\n",
      "        [-0.5000, -0.5000]])\n",
      "A = USV^T = tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-5.9605e-08,  1.0000e+00, -2.9802e-08]])\n"
     ]
    }
   ],
   "source": [
    "# singluar value decomposition\n",
    "\"\"\"\n",
    "U, S, V = torch.svd(A) returns the singular value decomposition of a real matrix A of size (n x m) \n",
    "such that A = USV^T.\n",
    "U is of shape (n×n).\n",
    "S is a diagonal matrix of shape (n×m), \n",
    "    represented as a vector of size min(n,m) containing the non-negative diagonal entries.\n",
    "V is of shape (m×m).\"\"\"\n",
    "A = torch.Tensor([[1,1,1],[0,1,0]])\n",
    "U,S,V = torch.svd(A)\n",
    "print(f'U = {U} \\n S = {S} \\n V = {V}')\n",
    "print(f'A = USV^T = {torch.chain_matmul(U,torch.diag(S),torch.transpose(V,0,1))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0.],\n",
       "          [1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 1.],\n",
       "          [0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0.]]]),\n",
       " tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.5793,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.1238, -0.4372,  1.0000,  0.0000,  0.0000],\n",
       "          [ 0.3117, -1.1657, -1.2497,  1.0000,  0.0000],\n",
       "          [ 0.3741,  0.1693, -0.5634, -0.1194,  1.0000]],\n",
       " \n",
       "         [[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0652,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0614,  0.1640,  1.0000,  0.0000,  0.0000],\n",
       "          [-0.7786,  1.5405,  0.8784,  1.0000,  0.0000],\n",
       "          [-1.6803, -0.4199, -0.4098,  1.9121,  1.0000]]]),\n",
       " tensor([[[-0.7550,  2.2018,  1.3260,  0.4933,  0.9644],\n",
       "          [ 0.0000, -0.5526,  0.0224, -0.5931, -0.5349],\n",
       "          [ 0.0000,  0.0000, -0.2805, -0.9726, -0.5421],\n",
       "          [ 0.0000,  0.0000,  0.0000, -0.9371,  0.9154],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3429]],\n",
       " \n",
       "         [[-0.4906, -0.4337,  1.0164,  0.6601, -0.5864],\n",
       "          [ 0.0000, -0.3340, -1.5395, -1.1347,  0.5114],\n",
       "          [ 0.0000,  0.0000, -0.6442, -2.0123, -0.7842],\n",
       "          [ 0.0000,  0.0000,  0.0000, -0.2717,  0.6395],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  2.1291]]]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower-triangular and Upper-triangular (LU) decomposition\n",
    "A = torch.randn(2,5,5)\n",
    "# computed packed decompositions and pivots\n",
    "B = torch.btrifact(A)\n",
    "# returns the unpacked versions, passes only the pivots\n",
    "torch.btriunpack(A,B[1])\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = tensor([[-0.0546,  0.0188,  0.0082],\n",
      "        [ 0.3140, -0.1597,  0.9358],\n",
      "        [-0.8903, -0.3889,  0.2330],\n",
      "        [ 0.3252, -0.9072, -0.2644]]) \n",
      " R = tensor([[-0.6418,  1.8647, -0.8099],\n",
      "        [ 0.0000,  1.4626,  0.0718],\n",
      "        [ 0.0000,  0.0000, -0.4858]])\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "\n",
    "\"\"\"\n",
    "Computes the QR decomposition of a matrix input, and returns matrices Q and R such that \n",
    "    input = Q R, with Q being an orthogonal matrix and R being an upper triangular matrix.\n",
    "This returns the thin (reduced) QR factorization.\"\"\"\n",
    "A = torch.randn(4,3)\n",
    "Q, R = torch.qr(A)\n",
    "print(f'Q = {Q} \\n R = {R}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# other decompositions implemented: torch.cholesky(), .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of linear equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution = tensor([-7.0601, 10.5317, -5.6973, -2.3638, -1.6744])\n"
     ]
    }
   ],
   "source": [
    "# the following functions solve systems of linear equations\n",
    "\"\"\"\n",
    "Batch LU solve.\n",
    "Returns the LU solve of the linear system Ax = bAx=b.\"\"\"\n",
    "A = torch.randn(1,5,5)\n",
    "# computed packed decompositions and pivots (used for the input of the solver)\n",
    "LU_pack  = torch.btrifact(A)\n",
    "# define the coefficient vector \n",
    "b = torch.Tensor([[1,2,3,4,5]])\n",
    "print(f'solution = {torch.btrisolve(b, *LU_pack)[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0370,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9423, -0.3037,  1.0000,  0.0000,  0.0000],\n",
       "         [-0.3712,  1.3627, -1.0872,  1.0000,  0.0000],\n",
       "         [-0.4867, -0.9462, -1.3935,  2.4712,  1.0000]],\n",
       "\n",
       "        [[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.7748,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.5980, -1.0407,  1.0000,  0.0000,  0.0000],\n",
       "         [ 2.1710, -0.4454,  0.0460,  1.0000,  0.0000],\n",
       "         [-0.5681, -0.1681,  0.6145,  0.0692,  1.0000]]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =torch.btriunpack(A,B[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common comparison operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.max(x) = 3;  torch.max(x, y) = tensor([[4, 2, 3]]);\n",
      "torch.min(x) = 1;  torch.min(x, y) = tensor([[1, 2, 1]]);\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3]])\n",
    "y = torch.tensor([[4,2,1]])\n",
    "# min - max\n",
    "print(f'torch.max(x) = {torch.max(x)}', end = ';  ') #tensor-wise\n",
    "print(f'torch.max(x, y) = {torch.max(x, y)}', end = ';\\n') #element-wise\n",
    "print(f'torch.min(x) = {torch.min(x)}', end = ';  ') \n",
    "print(f'torch.min(x, y) = {torch.min(x, y)}', end = ';\\n') \n",
    "# greater than (gt), greater equal (ge), lt, le, eq, not equal (ne),  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.min(x) = -3\n",
      "torch.min(x, y) = tensor([[[ 1,  2,  1],\n",
      "         [-1,  2, -3]]]);\n",
      "torch.min(y,x) = tensor([[[ 1,  2,  1],\n",
      "         [-1,  2, -3]]]);\n"
     ]
    }
   ],
   "source": [
    "### interesting properties due to numpy broadcasting semantics\n",
    "x = torch.tensor([[1,2,3],\n",
    "                  [-1,6,-3]])\n",
    "y = torch.tensor([4,2,1])\n",
    "print(f'torch.min(x) = {torch.min(x)}') \n",
    "print(f'torch.min(x, y) = {torch.min(x, y)}', end = ';\\n') \n",
    "print(f'torch.min(y,x) = {torch.min(y,x)}', end = ';\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.Tensor class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2,3],[2,4,6]])\n",
    "torch.matrix_rank(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
