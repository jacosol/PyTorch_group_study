{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLAS and LAPACK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"BLAS is a collection of low-level matrix and vector arithmetic operations (“multiply a vector by a scalar”, “multiply two matrices and add to a third matrix”, etc ...).\n",
    "\n",
    "LAPACK is a collection of higher-level linear algebra operations. Things like matrix factorizations (LU, LLt, QR, SVD, Schur, etc) that are used to do things like “find the eigenvalues of a matrix”, or “find the singular values of a matrix”, or “solve a linear system”. LAPACK is built on top of the BLAS; many users of LAPACK only use the LAPACK interfaces and never need to be aware of the BLAS at all. LAPACK is generally compiled separately from the BLAS, and can use whatever highly-optimized BLAS implementation you have available.\"\n",
    "\n",
    "from SO answer here https://stackoverflow.com/questions/17858104/what-is-the-relation-between-blas-lapack-and-atlas\\\n",
    "\n",
    "docs from the intel website https://software.intel.com/en-us/mkl-developer-reference-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix and vector algebra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm : tensor([[10., 10., 10.],\n",
      "        [10., 10., 10.]])\n",
      "bmm : tensor([[[10., 10., 10.],\n",
      "         [10., 10., 10.]],\n",
      "\n",
      "        [[10., 10., 10.],\n",
      "         [10., 10., 10.]],\n",
      "\n",
      "        [[10., 10., 10.],\n",
      "         [10., 10., 10.]],\n",
      "\n",
      "        [[10., 10., 10.],\n",
      "         [10., 10., 10.]]])\n",
      "addbmm : tensor([[90., 90., 90.],\n",
      "        [90., 90., 90.]])\n"
     ]
    }
   ],
   "source": [
    "# regular matrix multiplication\n",
    "mat1 = torch.ones(2,10)\n",
    "mat2 = torch.ones(10,3)\n",
    "res = torch.mm(mat1, mat2)\n",
    "print(f'mm : {res}')\n",
    "\n",
    "# multipling a batch of matrices\n",
    "M = torch.ones(2,3)*100\n",
    "bmat1 = torch.ones(4,2,10)\n",
    "bmat2 = torch.ones(4,10,3)\n",
    "res = torch.bmm(bmat1, bmat2)\n",
    "print(f'bmm : {res}')\n",
    "\n",
    "# multipling and adding together a batch of matrices then add a matrix to the end result\n",
    "M = torch.ones(2,3)*100\n",
    "bmat1 = torch.ones(4,2,10)\n",
    "bmat2 = torch.ones(4,10,3)\n",
    "res = torch.addbmm(M, bmat1, bmat2, alpha=1, beta=0.5)\n",
    "print(f'addbmm : {res}')\n",
    "\n",
    "# other variants are present like: torch.addmm, torch.baddbmm,  .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv : tensor([ 0.5911, -1.8514])\n",
      "addmv : tensor([8.6583, 8.2520])\n"
     ]
    }
   ],
   "source": [
    "# multiply a matrix and a vector\n",
    "mat = torch.randn(2, 3)\n",
    "vec = torch.ones(3)\n",
    "res = torch.mv(mat, vec)\n",
    "print(f'mv : {res}')\n",
    "\n",
    "# multiply a matrix and a vector and add another vector\n",
    "V = torch.ones(2)*10\n",
    "mat = torch.randn(2, 3)\n",
    "vec = torch.ones(3)\n",
    "res = torch.addmv(V, mat, vec)\n",
    "print(f'addmv : {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix*matrix :tensor([[-0.2968, -2.0897],\n",
      "        [ 1.0259,  0.2704]])\n",
      "\n",
      "vector*matrix :tensor([1.2198, 0.2322])\n",
      "\n",
      "matrix*vector :tensor([3.9841, 0.6078, 1.2510])\n",
      "\n",
      "vector*vector :-0.6373900771141052\n",
      "\n",
      "3Dtensor*vector :tensor([[-0.2895, -0.6748],\n",
      "        [-0.6835,  0.6867],\n",
      "        [ 0.3335,  0.9583],\n",
      "        [ 0.1476, -0.2026],\n",
      "        [-0.2862,  0.3676]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# matmul: multiplying tensors with any dimension (generalization)\n",
    "\"\"\"\n",
    "The behavior depends on the dimensionality of the tensors as follows:\n",
    "\n",
    "If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
    "If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
    "If the first argument is 1-dimensional and the second argument is 2-dimensional, \n",
    "    a 1 is prepended to its dimension for the purpose of the matrix multiply. \n",
    "    After the matrix multiply, the prepended dimension is removed.\n",
    "If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
    "    the matrix-vector product is returned.\n",
    "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), \n",
    "    then a batched matrix multiply is returned. If the first argument is 1-dimensional, \n",
    "    a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. \n",
    "    If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if tensor1 is a (j \\times 1 \\times n \\times m)(j×1×n×m) tensor and tensor2 is a (k \\times m \\times p)(k×m×p) tensor, out will be an (j \\times k \\times n \\times p)(j×k×n×p) tensor.\n",
    "\"\"\"\n",
    "tensor1 = torch.randn(2,3)\n",
    "tensor2 = torch.randn(3,2)\n",
    "print(f'matrix*matrix :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3,2)\n",
    "print(f'vector*matrix :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(3,2)\n",
    "tensor2 = torch.randn(2)\n",
    "print(f'matrix*vector :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "print(f'vector*vector :{torch.matmul(tensor1, tensor2)}', end='\\n\\n')\n",
    "tensor1 = torch.randn(5,2,3)\n",
    "tensor2 = torch.randn(3)\n",
    "print(f'3Dtensor*vector :{torch.matmul(tensor1, tensor2)}', end='\\n\\n') # also the other way around works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7495,  0.6446],\n",
      "        [-0.5327,  1.9050]])\n"
     ]
    }
   ],
   "source": [
    "# chain matrix multuplication\n",
    "mat1 = torch.randn(2,3)\n",
    "mat2 = torch.randn(3,4)\n",
    "mat3 = torch.randn(4,2)\n",
    "res = torch.chain_matmul(mat1, mat2, mat3)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outer product\n",
    "torch.ger(torch.Tensor([1,2]),torch.Tensor([3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other obvious function are present like: torch.matrix_power(input, n), torch.dot(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting matrix attributes (det, rank, eigenvalues..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.inverse\n",
    "torch.det \n",
    "torch.eig # eigenvalues and eigenvectors\n",
    "torch.matrix_rank\n",
    "# .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix decompositions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = tensor([[-0.9239, -0.3827],\n",
      "        [-0.3827,  0.9239]]) \n",
      " S = tensor([1.8478, 0.7654]) \n",
      " V = tensor([[-0.5000, -0.5000],\n",
      "        [-0.7071,  0.7071],\n",
      "        [-0.5000, -0.5000]])\n",
      "A = USV^T = tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n",
      "        [-5.9605e-08,  1.0000e+00, -2.9802e-08]])\n"
     ]
    }
   ],
   "source": [
    "# singluar value decomposition\n",
    "\"\"\"\n",
    "U, S, V = torch.svd(A) returns the singular value decomposition of a real matrix A of size (n x m) \n",
    "such that A = USV^T.\n",
    "U is of shape (n×n).\n",
    "S is a diagonal matrix of shape (n×m), \n",
    "    represented as a vector of size min(n,m) containing the non-negative diagonal entries.\n",
    "V is of shape (m×m).\"\"\"\n",
    "A = torch.Tensor([[1,1,1],[0,1,0]])\n",
    "U,S,V = torch.svd(A)\n",
    "print(f'U = {U} \\n S = {S} \\n V = {V}')\n",
    "print(f'A = USV^T = {torch.chain_matmul(U,torch.diag(S),torch.transpose(V,0,1))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0.],\n",
       "          [1., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 1.],\n",
       "          [0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0.]]]),\n",
       " tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.5793,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.1238, -0.4372,  1.0000,  0.0000,  0.0000],\n",
       "          [ 0.3117, -1.1657, -1.2497,  1.0000,  0.0000],\n",
       "          [ 0.3741,  0.1693, -0.5634, -0.1194,  1.0000]],\n",
       " \n",
       "         [[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0652,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0614,  0.1640,  1.0000,  0.0000,  0.0000],\n",
       "          [-0.7786,  1.5405,  0.8784,  1.0000,  0.0000],\n",
       "          [-1.6803, -0.4199, -0.4098,  1.9121,  1.0000]]]),\n",
       " tensor([[[-0.7550,  2.2018,  1.3260,  0.4933,  0.9644],\n",
       "          [ 0.0000, -0.5526,  0.0224, -0.5931, -0.5349],\n",
       "          [ 0.0000,  0.0000, -0.2805, -0.9726, -0.5421],\n",
       "          [ 0.0000,  0.0000,  0.0000, -0.9371,  0.9154],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3429]],\n",
       " \n",
       "         [[-0.4906, -0.4337,  1.0164,  0.6601, -0.5864],\n",
       "          [ 0.0000, -0.3340, -1.5395, -1.1347,  0.5114],\n",
       "          [ 0.0000,  0.0000, -0.6442, -2.0123, -0.7842],\n",
       "          [ 0.0000,  0.0000,  0.0000, -0.2717,  0.6395],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  2.1291]]]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower-triangular and Upper-triangular (LU) decomposition\n",
    "A = torch.randn(2,5,5)\n",
    "# computed packed decompositions and pivots\n",
    "B = torch.btrifact(A)\n",
    "# returns the unpacked versions, passes only the pivots\n",
    "torch.btriunpack(A,B[1])\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = tensor([[-0.0546,  0.0188,  0.0082],\n",
      "        [ 0.3140, -0.1597,  0.9358],\n",
      "        [-0.8903, -0.3889,  0.2330],\n",
      "        [ 0.3252, -0.9072, -0.2644]]) \n",
      " R = tensor([[-0.6418,  1.8647, -0.8099],\n",
      "        [ 0.0000,  1.4626,  0.0718],\n",
      "        [ 0.0000,  0.0000, -0.4858]])\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "\n",
    "\"\"\"\n",
    "Computes the QR decomposition of a matrix input, and returns matrices Q and R such that \n",
    "    input = Q R, with Q being an orthogonal matrix and R being an upper triangular matrix.\n",
    "This returns the thin (reduced) QR factorization.\"\"\"\n",
    "A = torch.randn(4,3)\n",
    "Q, R = torch.qr(A)\n",
    "print(f'Q = {Q} \\n R = {R}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# other decompositions implemented: torch.cholesky(), .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of linear equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution = tensor([-7.0601, 10.5317, -5.6973, -2.3638, -1.6744])\n"
     ]
    }
   ],
   "source": [
    "# the following functions solve systems of linear equations\n",
    "\"\"\"\n",
    "Batch LU solve.\n",
    "Returns the LU solve of the linear system Ax = bAx=b.\"\"\"\n",
    "A = torch.randn(1,5,5)\n",
    "# computed packed decompositions and pivots (used for the input of the solver)\n",
    "LU_pack  = torch.btrifact(A)\n",
    "# define the coefficient vector \n",
    "b = torch.Tensor([[1,2,3,4,5]])\n",
    "print(f'solution = {torch.btrisolve(b, *LU_pack)[0]}')\n",
    "\n",
    "# other variants on the same flavour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common comparison operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.max(x) = 3;  torch.max(x, y) = tensor([[4, 2, 3]]);\n",
      "torch.min(x) = 1;  torch.min(x, y) = tensor([[1, 2, 1]]);\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3]])\n",
    "y = torch.tensor([[4,2,1]])\n",
    "# min - max\n",
    "print(f'torch.max(x) = {torch.max(x)}', end = ';  ') #tensor-wise\n",
    "print(f'torch.max(x, y) = {torch.max(x, y)}', end = ';\\n') #element-wise\n",
    "print(f'torch.min(x) = {torch.min(x)}', end = ';  ') \n",
    "print(f'torch.min(x, y) = {torch.min(x, y)}', end = ';\\n') \n",
    "# greater than (gt), greater equal (ge), lt, le, eq, not equal (ne), all element-wise if same shape or see below\n",
    "# torch.equal returns bool if shape and elements are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.min(x) = -3\n",
      "torch.min(x, y) = tensor([[[ 1,  2,  1],\n",
      "         [-1,  2, -3]]]);\n",
      "torch.min(y,x) = tensor([[[ 1,  2,  1],\n",
      "         [-1,  2, -3]]]);\n"
     ]
    }
   ],
   "source": [
    "### interesting properties due to numpy broadcasting semantics\n",
    "x = torch.tensor([[1,2,3],\n",
    "                  [-1,6,-3]])\n",
    "y = torch.tensor([4,2,1])\n",
    "print(f'torch.min(x) = {torch.min(x)}') \n",
    "print(f'torch.min(x, y) = {torch.min(x, y)}', end = ';\\n') \n",
    "print(f'torch.min(y,x) = {torch.min(y,x)}', end = ';\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allclose and sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### allclose between two tensor element wise\n",
    "\"\"\"\n",
    "∣self−other∣≤ atol + rtol×∣other∣\n",
    "\"\"\"\n",
    "self = torch.tensor([10,0.001])\n",
    "other = torch.tensor([10.01, 0.011])\n",
    "torch.allclose(self, other, rtol=1e-01, atol=1e-01, equal_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4570, -1.0176, -1.7433],\n",
      "        [ 1.0485, -0.4385,  0.2557]])\n",
      "(tensor([[-1.7433, -1.0176, -0.4570],\n",
      "        [-0.4385,  0.2557,  1.0485]]), tensor([[2, 1, 0],\n",
      "        [1, 2, 0]]))\n"
     ]
    }
   ],
   "source": [
    "### returns tuple of sorted tensor and sorting indices along a given dimension\n",
    "a = torch.randn(2, 3)\n",
    "print(a)\n",
    "print(torch.sort(a, dim =1)) # sort each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### very useful comparison operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2802,  0.7261, -0.4381,  1.2156],\n",
      "        [ 0.1427, -0.1381,  1.0498,  0.6352],\n",
      "        [-0.7170, -0.1572, -0.4860,  0.6571]])\n",
      "topk = (tensor([[ 1.2156,  1.0498,  0.6571],\n",
      "        [ 0.7261,  0.6352, -0.1572]]), tensor([[3, 2, 3],\n",
      "        [1, 3, 1]]))\n",
      "kthvalue = (tensor([-0.7170, -0.1572, -0.4860,  0.6352]), tensor([2, 2, 2, 1]))\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(3,4)\n",
    "print(tensor)\n",
    "# topk to find the largest value in a tensor (always used for categorical features)\n",
    "print(f'topk = {torch.topk(tensor.t(), k = 2, dim=0, largest=True, sorted=True, out=None)}') # k is how many largest\n",
    "\n",
    "# returns the smallest k \n",
    "print(f'kthvalue = {torch.kthvalue(tensor.t(), k=1, dim=1, keepdim=False, out=None)}') # k is the kth smallest\n",
    "\n",
    "# note that dim works differently than in sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4039e+00, -1.5218e-01,  4.9487e-01],\n",
      "         [ 7.0787e-01, -2.2500e-01,  1.3149e-01],\n",
      "         [-2.0341e-01,  8.5648e-01,  2.4924e-01]],\n",
      "\n",
      "        [[ 2.1495e+00,  1.0697e-03,  4.5938e-01],\n",
      "         [-8.6795e-01, -1.3056e-01,  6.0574e-01],\n",
      "         [-1.4216e+00,  1.3023e+00,  7.8987e-01]]])\n",
      "\n",
      "roll the subtensors\n",
      "tensor([[[ 2.1495e+00,  1.0697e-03,  4.5938e-01],\n",
      "         [-8.6795e-01, -1.3056e-01,  6.0574e-01],\n",
      "         [-1.4216e+00,  1.3023e+00,  7.8987e-01]],\n",
      "\n",
      "        [[ 1.4039e+00, -1.5218e-01,  4.9487e-01],\n",
      "         [ 7.0787e-01, -2.2500e-01,  1.3149e-01],\n",
      "         [-2.0341e-01,  8.5648e-01,  2.4924e-01]]])\n",
      "\n",
      "roll the rows in the subtensors\n",
      "tensor([[[-2.0341e-01,  8.5648e-01,  2.4924e-01],\n",
      "         [ 1.4039e+00, -1.5218e-01,  4.9487e-01],\n",
      "         [ 7.0787e-01, -2.2500e-01,  1.3149e-01]],\n",
      "\n",
      "        [[-1.4216e+00,  1.3023e+00,  7.8987e-01],\n",
      "         [ 2.1495e+00,  1.0697e-03,  4.5938e-01],\n",
      "         [-8.6795e-01, -1.3056e-01,  6.0574e-01]]])\n",
      "\n",
      "roll the columns in subtensors\n",
      "tensor([[[ 4.9487e-01,  1.4039e+00, -1.5218e-01],\n",
      "         [ 1.3149e-01,  7.0787e-01, -2.2500e-01],\n",
      "         [ 2.4924e-01, -2.0341e-01,  8.5648e-01]],\n",
      "\n",
      "        [[ 4.5938e-01,  2.1495e+00,  1.0697e-03],\n",
      "         [ 6.0574e-01, -8.6795e-01, -1.3056e-01],\n",
      "         [ 7.8987e-01, -1.4216e+00,  1.3023e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# almost all these operations are pretty self-explanatory but here I mention few of them that could be tricky \n",
    "#or deserve a bit of explanation\n",
    "\n",
    "tensor = torch.randn(2,3,3)\n",
    "print(tensor)\n",
    "print()\n",
    "print('roll the subtensors')\n",
    "print(torch.roll(tensor, 1, dims = 0))\n",
    "print()\n",
    "print('roll the rows in the subtensors')\n",
    "print(torch.roll(tensor, 1, dims = 1))\n",
    "print()\n",
    "print('roll the columns in subtensors')\n",
    "print(torch.roll(tensor, 1, dims = 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.Tensor class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the functions from torch are implemented both as in-place and not in-place methods in the torch.Tensor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3846,  1.5621,  0.1579],\n",
      "         [-0.2362,  0.5538, -0.8171]],\n",
      "\n",
      "        [[ 0.0506, -1.0614,  1.8333],\n",
      "         [ 0.0219, -0.2927,  1.3855]]])\n",
      "tensor([[[ 999.6154, 1001.5621, 1000.1579],\n",
      "         [ 999.7638, 1000.5538,  999.1829]],\n",
      "\n",
      "        [[1000.0506,  998.9386, 1001.8333],\n",
      "         [1000.0219,  999.7073, 1001.3856]]])\n"
     ]
    }
   ],
   "source": [
    "# inplace tensor version of map\n",
    "def add_1000(x,y):\n",
    "    return x + 1000\n",
    "tensor1 = torch.randn(2,2,3)\n",
    "print(tensor1)\n",
    "tensor1.map_(tensor1,add_1000)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeating a tensor (similar to numpy.tile)\n",
    "\"\"\"\n",
    "Signature: np.tile(A, reps)\n",
    "Docstring:\n",
    "Construct an array by repeating A the number of times given by reps.\n",
    "Parameters\n",
    "----------\n",
    "A : array_like\n",
    "    The input array.\n",
    "reps : array_like\n",
    "    The number of repetitions of `A` along each axis.\n",
    "\"\"\"\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x)\n",
    "print(x.repeat(4, 2))\n",
    "x.repeat(4, 2, 1).size()\n",
    "torch.Size([4, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on broadcasting semantics\n",
    "https://pytorch.org/docs/stable/notes/broadcasting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
