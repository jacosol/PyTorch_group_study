{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/CNN_0.png' height=50% width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layers\n",
    "### Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D creates a convolutional layer that can be implemented in a nerual network.  \n",
    "When the forward method is called, it computes valid cross-correlation function between input (N, C_in, L) and weight according to \n",
    "<img src='img/CNN_0_formula1D.png' height=50% width=50% />\n",
    "where N is the batch size, C_out is the size of the output and L is the lenght of th einput sequence.\n",
    "the operator * computes the cross-correlation (the difference with convolution is the sign of the varible we are summing on).\n",
    "Note that the mode used by np.convolve is \"full\" so no boundary effects are present (the cc is computed only for those points where the two signals overlap completely).\n",
    "\n",
    "There is the summation over input channels because the output number of channels has to correspond by definition to the number of filters (output channels i.e. number of weights) we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = nn.Conv1d(in_channels=2,out_channels=3,kernel_size=2)\n",
    "x1 = torch.rand(1,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5263, 0.5980, 0.2248, 0.5857],\n",
       "         [0.8853, 0.5427, 0.9346, 0.7085]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4770,  0.4113,  0.6521],\n",
       "         [-0.9659, -0.6491, -0.9817],\n",
       "         [ 0.0842,  0.1137, -0.0416]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optional arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stride: is the step at which the cross-correlations are computed. \n",
    "\n",
    "padding: controls the amount of implicit zero-paddings on both sides for padding number of points.\n",
    "\n",
    "dilation: \n",
    "\n",
    "groups: controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,\n",
    "\n",
    "At groups=1, all inputs are convolved to all outputs.\n",
    "At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.\n",
    "At groups= in_channels, each input channel is convolved with its own set of filters of size C_out/C_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1068,  0.3042],\n",
      "         [ 0.2017, -0.0955],\n",
      "         [-0.4905, -0.1482],\n",
      "         [-0.5387, -0.5528],\n",
      "         [-0.3839, -0.0224],\n",
      "         [ 0.0204,  0.0375],\n",
      "         [ 0.9061,  0.8317],\n",
      "         [-0.2352,  0.2851]],\n",
      "\n",
      "        [[ 0.2415,  0.2527],\n",
      "         [ 0.0141,  0.0045],\n",
      "         [-0.3263, -0.1666],\n",
      "         [-0.3543, -0.2868],\n",
      "         [-0.2933, -0.1523],\n",
      "         [ 0.1614,  0.0804],\n",
      "         [ 0.6538,  0.6598],\n",
      "         [-0.2332,  0.0757]],\n",
      "\n",
      "        [[ 0.1794,  0.1971],\n",
      "         [-0.3434, -0.0297],\n",
      "         [-0.2897, -0.1987],\n",
      "         [-0.3667, -0.5489],\n",
      "         [-0.1785, -0.1506],\n",
      "         [ 0.2972, -0.0150],\n",
      "         [ 0.5186,  0.5807],\n",
      "         [-0.1067,  0.1523]]], grad_fn=<SqueezeBackward1>)\n",
      "tensor([[[-0.1658, -0.0269],\n",
      "         [ 0.6533,  1.1846],\n",
      "         [-0.0983, -0.1844],\n",
      "         [-0.0409, -0.2461],\n",
      "         [-0.4755, -0.7427],\n",
      "         [ 0.6401,  0.3461],\n",
      "         [ 1.0595,  1.0348],\n",
      "         [ 0.1137,  0.2515]],\n",
      "\n",
      "        [[-0.2009, -0.2061],\n",
      "         [ 0.7570,  0.6576],\n",
      "         [-0.2207, -0.2742],\n",
      "         [-0.0837, -0.0915],\n",
      "         [-0.4921, -0.8075],\n",
      "         [ 0.6184,  0.2884],\n",
      "         [ 0.9559,  0.8429],\n",
      "         [ 0.2573,  0.2391]],\n",
      "\n",
      "        [[-0.1533, -0.1798],\n",
      "         [ 1.0085,  0.6836],\n",
      "         [-0.1000, -0.2013],\n",
      "         [-0.2506, -0.1888],\n",
      "         [-0.5041, -0.7622],\n",
      "         [ 0.5933,  0.3164],\n",
      "         [ 0.8889,  0.7233],\n",
      "         [ 0.3830, -0.0492]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(3,4,3)\n",
    "cl = nn.Conv1d(4,8,2, groups=1)\n",
    "print(cl(x1))\n",
    "cl = nn.Conv1d(in_channels=4,out_channels=8,kernel_size=2, groups=4)\n",
    "print(cl(x1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1415, -0.4303, -0.4541, -0.2835],\n",
      "         [ 0.5674,  0.6165,  0.8222,  0.7672],\n",
      "         [ 0.3145,  0.1210,  0.2560,  0.2406],\n",
      "         [-0.3722, -0.3997, -0.5523, -0.1516],\n",
      "         [-0.3507,  0.1722, -0.3360, -0.5153],\n",
      "         [-0.0526, -0.1175, -0.2858, -0.1980],\n",
      "         [ 0.6233,  0.8496,  0.6187,  0.4336],\n",
      "         [ 0.0950,  0.0949,  0.4148,  0.1799]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(1,4,8)\n",
    "cl = nn.Conv1d(in_channels=4,out_channels=8,kernel_size=2, stride = 2)\n",
    "print(cl(x1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0363,  0.0363,  0.0363,  0.2533,  0.4009,  0.5153,  0.1061,\n",
      "           0.0363,  0.0363,  0.0363],\n",
      "         [ 0.1866,  0.1866,  0.1866,  0.3515,  0.3384, -0.1029, -0.0934,\n",
      "           0.1866,  0.1866,  0.1866],\n",
      "         [ 0.1952,  0.1952,  0.1952,  0.2486,  0.1768,  0.1418,  0.1937,\n",
      "           0.1952,  0.1952,  0.1952],\n",
      "         [ 0.1495,  0.1495,  0.1495,  0.1241, -0.3178, -0.4035, -0.3329,\n",
      "           0.1495,  0.1495,  0.1495],\n",
      "         [ 0.2479,  0.2479,  0.2479,  0.4192,  0.4596,  0.9543,  0.2493,\n",
      "           0.2479,  0.2479,  0.2479],\n",
      "         [-0.3109, -0.3109, -0.3109, -0.2959, -0.1181,  0.0879, -0.3229,\n",
      "          -0.3109, -0.3109, -0.3109],\n",
      "         [ 0.0586,  0.0586,  0.0586,  0.2762,  0.6505,  0.6630,  0.2378,\n",
      "           0.0586,  0.0586,  0.0586],\n",
      "         [ 0.1671,  0.1671,  0.1671,  0.2336,  0.2048,  0.1970,  0.3639,\n",
      "           0.1671,  0.1671,  0.1671]],\n",
      "\n",
      "        [[ 0.0363,  0.0363,  0.0363,  0.2708,  0.3443,  0.2009,  0.6032,\n",
      "           0.0363,  0.0363,  0.0363],\n",
      "         [ 0.1866,  0.1866,  0.1866,  0.6061,  0.1245,  0.3852, -0.1975,\n",
      "           0.1866,  0.1866,  0.1866],\n",
      "         [ 0.1952,  0.1952,  0.1952,  0.5407,  0.3209,  0.2766,  0.2932,\n",
      "           0.1952,  0.1952,  0.1952],\n",
      "         [ 0.1495,  0.1495,  0.1495,  0.2312, -0.1173, -0.3917, -0.3596,\n",
      "           0.1495,  0.1495,  0.1495],\n",
      "         [ 0.2479,  0.2479,  0.2479,  0.3989,  0.8522,  0.5987,  0.7986,\n",
      "           0.2479,  0.2479,  0.2479],\n",
      "         [-0.3109, -0.3109, -0.3109, -0.1857,  0.2517,  0.2062,  0.2693,\n",
      "          -0.3109, -0.3109, -0.3109],\n",
      "         [ 0.0586,  0.0586,  0.0586,  0.2706,  0.5811,  0.7676,  0.1830,\n",
      "           0.0586,  0.0586,  0.0586],\n",
      "         [ 0.1671,  0.1671,  0.1671,  0.1879,  0.0794,  0.3245, -0.2124,\n",
      "           0.1671,  0.1671,  0.1671]],\n",
      "\n",
      "        [[ 0.0363,  0.0363,  0.0363,  0.2562,  0.6322,  0.3180,  0.3216,\n",
      "           0.0363,  0.0363,  0.0363],\n",
      "         [ 0.1866,  0.1866,  0.1866,  0.6880, -0.1313,  0.1603, -0.1181,\n",
      "           0.1866,  0.1866,  0.1866],\n",
      "         [ 0.1952,  0.1952,  0.1952,  0.3495,  0.3001,  0.2740,  0.1591,\n",
      "           0.1952,  0.1952,  0.1952],\n",
      "         [ 0.1495,  0.1495,  0.1495,  0.0143, -0.5008, -0.3967, -0.2620,\n",
      "           0.1495,  0.1495,  0.1495],\n",
      "         [ 0.2479,  0.2479,  0.2479,  0.4141,  1.0926,  0.4945,  0.5781,\n",
      "           0.2479,  0.2479,  0.2479],\n",
      "         [-0.3109, -0.3109, -0.3109, -0.2074,  0.2827, -0.1076,  0.0566,\n",
      "          -0.3109, -0.3109, -0.3109],\n",
      "         [ 0.0586,  0.0586,  0.0586,  0.5523,  0.6924,  0.5736,  0.2939,\n",
      "           0.0586,  0.0586,  0.0586],\n",
      "         [ 0.1671,  0.1671,  0.1671,  0.3673,  0.1468,  0.3755, -0.0296,\n",
      "           0.1671,  0.1671,  0.1671]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(3,4,3)\n",
    "cl = nn.Conv1d(in_channels=4,out_channels=8,kernel_size=2, padding = 4)\n",
    "print(cl(x1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layers\n",
    "### MaxPool1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = \n",
      "tensor([[[-0.3541, -0.2687, -0.4812,  1.0095, -0.0977],\n",
      "         [ 1.2322, -0.6976,  0.3482, -0.5973,  0.2829],\n",
      "         [-0.2630, -0.0045,  0.2014,  1.8985, -0.6775]],\n",
      "\n",
      "        [[-0.2400,  1.4647, -0.0685, -0.2705, -0.6889],\n",
      "         [-0.3335,  0.9157,  1.3174,  0.6771, -1.8515],\n",
      "         [-2.2814,  2.1701, -0.3036, -0.0586, -1.4421]]])\n",
      "\n",
      "output = \n",
      "tensor([[[-0.2687, -0.2687,  1.0095,  1.0095],\n",
      "         [ 1.2322,  0.3482,  0.3482,  0.2829],\n",
      "         [-0.0045,  0.2014,  1.8985,  1.8985]],\n",
      "\n",
      "        [[ 1.4647,  1.4647, -0.0685, -0.2705],\n",
      "         [ 0.9157,  1.3174,  1.3174,  0.6771],\n",
      "         [ 2.1701,  2.1701, -0.0586, -0.0586]]])\n",
      "output = \n",
      "tensor([[[1, 1, 3, 3],\n",
      "         [0, 2, 2, 4],\n",
      "         [1, 2, 3, 3]],\n",
      "\n",
      "        [[1, 1, 2, 3],\n",
      "         [1, 2, 2, 3],\n",
      "         [1, 1, 3, 3]]])\n"
     ]
    }
   ],
   "source": [
    "# pretty easy and self-explanatory. Also generalization for MaxPool2d is straightforward.\n",
    "m = nn.MaxPool1d(2, stride=1, return_indices=True)\n",
    "inp = torch.randn(2, 3, 5)\n",
    "print(f'input = \\n{inp}\\n')\n",
    "output = m(inp)\n",
    "print(f'output = \\n{output[0]}')\n",
    "print(f'output = \\n{output[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxUnpool1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3., 4., 5., 6., 7., 8.]]])\n",
      "tensor([[[2., 4., 6., 8.]]])\n",
      "tensor([[[1, 3, 5, 7]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 2., 0., 4., 0., 6., 0., 8.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Computes a partial inverse of MaxPool1d.\n",
    "\n",
    "MaxPool1d is not fully invertible, since the non-maximal values are lost.\n",
    "\n",
    "MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal\n",
    "values and computes a partial inverse in which all non-maximal values are set to zero.\n",
    "\"\"\"\n",
    "pool = nn.MaxPool1d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool1d(2, stride=2)\n",
    "input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])\n",
    "output, indices = pool(input)\n",
    "print(input)\n",
    "print(output)\n",
    "print(indices)\n",
    "unpool(output, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AvgPool1d same as max but with average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FractionalMaxPool2d there is a paper about this operation... \n",
    "#   https://arxiv.org/abs/1412.6071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nas far as the term “Adaptive” is concerned, it makes the following two lines of code equivalent:\\nAvgPool2d(kernel_size=7, stride=7, padding=0) //Here the three parameters \\nensure that the output activation has1by1 dimension\\nnn.AdaptiveAvgPool2d(1) //Here we don’t specify the kernel_size, stride or padding. \\nInstead, we specify the output dimension i.e 1by1\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### AdaptiveMaxPool : somehow equivalent to normal maxpool with some different argument to control the output dimension\n",
    "\"\"\"\n",
    "as far as the term “Adaptive” is concerned, it makes the following two lines of code equivalent:\n",
    "AvgPool2d(kernel_size=7, stride=7, padding=0) //Here the three parameters \n",
    "ensure that the output activation has1by1 dimension\n",
    "nn.AdaptiveAvgPool2d(1) //Here we don’t specify the kernel_size, stride or padding. \n",
    "Instead, we specify the output dimension i.e 1by1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding Layers\n",
    "### ReflectionPad1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[33., 22.,  1., 22., 33.,  4.,  5.,  4., 33.]]])\n"
     ]
    }
   ],
   "source": [
    "ref = nn.ReflectionPad1d(2)\n",
    "inp = torch.Tensor([[[1,22,33,4,5]]])\n",
    "out = ref(inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplicationPad1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  8.,  8.],\n",
      "         [11., 11., 11., 12., 13., 14., 15., 16., 17., 18., 18., 18.]]])\n"
     ]
    }
   ],
   "source": [
    "#replicates the last and first value of each 1d tensor\n",
    "\n",
    "ref = nn.ReplicationPad1d(2)\n",
    "inp = torch.Tensor([[[1,2,3,4,5,6,7,8],[11,12,13,14,15,16,17,18]]])\n",
    "out = ref(inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConstantPad1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.5000,  3.5000,  1.6734,  0.1705, -1.0744,  0.3329,  3.5000,\n",
       "           3.5000],\n",
       "         [ 3.5000,  3.5000, -0.0948, -0.6856,  0.1942, -0.3150,  3.5000,\n",
       "           3.5000]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.ConstantPad1d(2, 3.5)\n",
    "input = torch.randn(1, 2, 4)\n",
    "input\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Simple example of a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "# define the structure of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Dataset object by loading MNIST and assigning transform\n",
    "mnist_trainset = datasets.MNIST(root='./data', \n",
    "                                train=True, \n",
    "                                download=True, \n",
    "                                transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train loader for training\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=mnist_trainset,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = next(iter(train_loader))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADx9JREFUeJzt3X2wVPV9x/HPl8sFFDSR8CDiVcRn1AHNLYaxtaQUiw8t2hlUpk2IcXLtjHaaxmlr+SdmahIaExMmVdtLJGISn1o1oDFRh2RGjQnlamx8oAKxVAkI8pCAbQXu5ds/7l7nAvf8dtk9u2e53/drhtnd8z1nz3d2+Nyzu7+z52fuLgDxDCm6AQDFIPxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ia2sidDbPhPkIjG7lLIJT39T/a63usknVrCr+ZzZG0WFKLpG+5+6LU+iM0UhfarFp2CSBhla+seN2q3/abWYukOyVdKmmKpPlmNqXa5wPQWLV85p8uab27v+nueyU9KGluPm0BqLdawj9R0tv9Hm8sLTuAmXWYWZeZde3Tnhp2ByBPtYR/oC8VDvl9sLt3unu7u7e3angNuwOQp1rCv1FSW7/HJ0raVFs7ABqllvCvlnS6mZ1iZsMkXStpRT5tAai3qof63L3bzG6S9JR6h/qWuvtruXUGoK5qGud39yclPZlTLwAaiNN7gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiqoVN048gzdNJJyfq6jkNmaDuAJw4v6z55dzUtfeCOHZOT9e/cNSezNu6uF2ra92DAkR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgjJ3r35jsw2SdkvqkdTt7u2p9Y+10X6hzap6fzh8Q6aenay/0XFssv745d9I1s9qHX7YPTXKe74ns3bZ5/46ue2oh3+edzsNscpXapfvsErWzeMkn4+7+7YcngdAA/G2Hwiq1vC7pKfN7EUz68ijIQCNUevb/ovcfZOZjZP0jJn9p7s/23+F0h+FDkkaoaNr3B2AvNR05Hf3TaXbrZIekzR9gHU63b3d3dtb1bxfDgHRVB1+MxtpZsf03Zd0iaRX82oMQH3V8rZ/vKTHzKzvee539x/l0hWAuqs6/O7+pqSpOfaCDPY75yXrl3z7p5m1c0f8W3LbWUdlj4X3qu2j2uKdp2XWfrjlnOS2j535r8n6UTYsWR9l2b33pDcNgaE+ICjCDwRF+IGgCD8QFOEHgiL8QFBcursJbLhtRrJ++zXLkvXLj36v6n2v3fd+sv4v2y9O1p/+/iEndR5g0oPvZNZak1tKv1nZnawf1ZIer9vQ/b+ZtRE7e8rsffDjyA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHO3wA+I/3L5y/MezBZr2Ucv+Pt9Dj9+tumJOsjnvj3ZL1N6amueyz7KtJrl3w0ue2Eltou+3bb5ksza8N/sLqm5x4MOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM8zfAuxeMTNbnjdpe0/P/8dorMmu7/qktue3IJ1bVtO9yWsaMyaytv7Szpuf+P9+brK96PPuS5+XOT4iAIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFV2nN/Mlkq6QtJWdz+3tGy0pIckTZK0QdLV7r6zfm0e2U545FfJ+lkn31jT85+6MPu36SO7N9X03OW0jB2brJ/z1La67XvG4s8l6223M5afUsmR/15Jcw5adoukle5+uqSVpccAjiBlw+/uz0racdDiuZL6ppFZJunKnPsCUGfVfuYf7+6bJal0Oy6/lgA0Qt3P7TezDkkdkjRCtV2TDUB+qj3ybzGzCZJUut2ataK7d7p7u7u3t2p4lbsDkLdqw79C0oLS/QWSlufTDoBGKRt+M3tA0s8knWlmG83sekmLJM02s3WSZpceAziClP3M7+7zM0qzcu5l0Op+Z0uyPvlv0/VyvKat04a2nZisT1n+62R90fgXq973GT++Plk/6/4NyXp31XuOgTP8gKAIPxAU4QeCIvxAUIQfCIrwA0Fx6W4k/WbJsGS9lqG8037Ukayf8ZlfJOvd+3uq3jc48gNhEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIzzD3JDJ09K1m3pnmT9u6fcV2YP6UuzLd55WmZtym3vJrdlHL++OPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM8w8Cqctrn/zQO8ltv3lCuWms0+P4/7j97GT9+XnnZdZ6/mt9mX2jnjjyA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQZcf5zWyppCskbXX3c0vLbpX0GUl9P8he6O5P1qvJ6FrGjk3WU9Nk13JdfamCcfxrpybrPW+srWn/qJ9Kjvz3SpozwPKvu/u00j+CDxxhyobf3Z+VtKMBvQBooFo+899kZr80s6VmdlxuHQFoiGrDf7ekUyVNk7RZ0teyVjSzDjPrMrOufUpfLw5A41QVfnff4u497r5f0hJJ0xPrdrp7u7u3t2p4tX0CyFlV4TezCf0eXiXp1XzaAdAolQz1PSBppqQxZrZR0uclzTSzaZJc0gZJN9SxRwB1UDb87j5/gMX31KGXsN79ixnJ+qyOnyfrtYzlL/ltW7Ke+j2+xDj+kYwz/ICgCD8QFOEHgiL8QFCEHwiK8ANBcenuBhh6/PhkffYNP0vWvzTupTzbOcDT26Yk6z1vDM7La6cudy5Jeyelf0Y95Llf5NlOITjyA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPPnYN8ffjRZX7J0cbI+sSU9DXY9fXfyD5L1P/rTv0zWj350VdX77v6D9Ov2zoXpKz998dP3JetTh2VPT95qyU01zNIr/P63/iZZP+kL5aY+Lx5HfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IinH+Cu2+5mOZtfNu/o/ktkWO45cz3NL/Bf7h9iXJ+qf/5Lpk/UszHs2sTRmeviT5Oa3DkvXyqn/dr3trZrJ+yj+nr3PQU/WeG4cjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVXac38zaJN0n6XhJ+yV1uvtiMxst6SFJkyRtkHS1u++sX6u1aTnztGT9V3+evk77c5/6ambtI0OOqqqnPvfuOiFZX7T8qmS97cf7Mmvnfzl9zf+vHN+VrP/eiO5kfd3s9HkAabWO46et3fd+Zu261z+Z3PYjN2RvK0k9WzZW1VMzqeTI3y3pZnc/W9LHJN1oZlMk3SJppbufLmll6TGAI0TZ8Lv7Znd/qXR/t6Q1kiZKmitpWWm1ZZKurFeTAPJ3WJ/5zWySpPMlrZI03t03S71/ICSNy7s5APVTcfjNbJSkRyR91t13HcZ2HWbWZWZd+7Snmh4B1EFF4TezVvUG/3vu3vdLjS1mNqFUnyBp60Dbununu7e7e3ur0hdkBNA4ZcNvZibpHklr3P2OfqUVkhaU7i+QtDz/9gDUSyU/6b1I0ickvWJmL5eWLZS0SNLDZna9pLckzatPi/lYc/Nxyfr6y+8s8wzVD+d9eXt6GuwXrjkvWZ+8Jj2Fd8pPx2X/FFmS7vr77cn63GNeS9ZHD0n/F/r2b89M1lPufO3iZP3DK0Ym6yO2Z/+w9kM/XJ3cNj3AOTiUDb+7Py8p6yLms/JtB0CjcIYfEBThB4Ii/EBQhB8IivADQRF+IChz94bt7Fgb7RdaMaODT216OVnv8f3J+q792T/xvH1beiz95T87K73v19cm60VKXbJckvZ8KD2V9ZjO6s9RwOFb5Su1y3eUmYC8F0d+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwgqzDh/y0/Sl8d+/IwnkvWp37wpszZx0QtV9QTkjXF+AGURfiAowg8ERfiBoAg/EBThB4Ii/EBQlVy3f1Do+fimZP0yXZCsTxRj+RhcOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFBlw29mbWb2EzNbY2avmdlflZbfama/NrOXS/8uq3+7APJSyUk+3ZJudveXzOwYSS+a2TOl2tfd/av1aw9AvZQNv7tvlrS5dH+3ma2RNLHejQGor8P6zG9mkySdL2lVadFNZvZLM1tqZsdlbNNhZl1m1rVPe2pqFkB+Kg6/mY2S9Iikz7r7Lkl3SzpV0jT1vjP42kDbuXunu7e7e3urhufQMoA8VBR+M2tVb/C/5+6PSpK7b3H3HnffL2mJpOn1axNA3ir5tt8k3SNpjbvf0W/5hH6rXSXp1fzbA1AvlXzbf5GkT0h6xcz65rleKGm+mU2T5JI2SLqhLh0CqItKvu1/XtJA1wF/Mv92ADQKZ/gBQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCMndv3M7M3pX03/0WjZG0rWENHJ5m7a1Z+5LorVp59nayu4+tZMWGhv+QnZt1uXt7YQ0kNGtvzdqXRG/VKqo33vYDQRF+IKiiw99Z8P5TmrW3Zu1LordqFdJboZ/5ARSn6CM/gIIUEn4zm2Nmb5jZejO7pYgespjZBjN7pTTzcFfBvSw1s61m9mq/ZaPN7BkzW1e6HXCatIJ6a4qZmxMzSxf62jXbjNcNf9tvZi2S1kqaLWmjpNWS5rv76w1tJIOZbZDU7u6Fjwmb2cWS3pN0n7ufW1r2FUk73H1R6Q/nce7+d03S262S3it65ubShDIT+s8sLelKSZ9Sga9doq+rVcDrVsSRf7qk9e7+prvvlfSgpLkF9NH03P1ZSTsOWjxX0rLS/WXq/c/TcBm9NQV33+zuL5Xu75bUN7N0oa9doq9CFBH+iZLe7vd4o5prym+X9LSZvWhmHUU3M4DxpWnT+6ZPH1dwPwcrO3NzIx00s3TTvHbVzHidtyLCP9DsP8005HCRu18g6VJJN5be3qIyFc3c3CgDzCzdFKqd8TpvRYR/o6S2fo9PlLSpgD4G5O6bSrdbJT2m5pt9eEvfJKml260F9/OBZpq5eaCZpdUEr10zzXhdRPhXSzrdzE4xs2GSrpW0ooA+DmFmI0tfxMjMRkq6RM03+/AKSQtK9xdIWl5gLwdolpmbs2aWVsGvXbPNeF3IST6loYxvSGqRtNTdv9jwJgZgZpPVe7SXeicxvb/I3szsAUkz1furry2SPi/p+5IelnSSpLckzXP3hn/xltHbTPW+df1g5ua+z9gN7u13JT0n6RVJ+0uLF6r383Vhr12ir/kq4HXjDD8gKM7wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1P8D1fIoGOQBcvQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.numpy()[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0465,  0.1039,  0.0145, -0.0196, -0.0103,  0.0055, -0.0383,  0.0069,\n",
      "         -0.0028, -0.0145]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(model(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
