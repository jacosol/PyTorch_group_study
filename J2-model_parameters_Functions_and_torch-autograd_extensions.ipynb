{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# che vor di\n",
    "\"\"\"If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient,\n",
    "the function additionally requires specifying grad_tensors. It should be a sequence of matching length, \n",
    "that contains gradient of the differentiated function w.r.t. corresponding tensors \n",
    "(None is an acceptable value for all tensors that don’t need gradient tensors)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "CHILDREN 1 IS :\n",
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "\n",
      "\n",
      "size of parameters for child 1 are :\n",
      "torch.Size([64, 3, 7, 7])\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "CHILDREN 2 IS :\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "\n",
      "\n",
      "size of parameters for child 2 are :\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "CHILDREN 3 IS :\n",
      "ReLU(inplace)\n",
      "\n",
      "\n",
      "size of parameters for child 3 are :\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "CHILDREN 4 IS :\n",
      "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "\n",
      "\n",
      "size of parameters for child 4 are :\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "CHILDREN 5 IS :\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "size of parameters for child 5 are :\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A pytorch model has the parameters() method that returns every single tensors composing the network.\n",
    "A pytorch model has the children() method that returns all the layers of the model.\n",
    "Each child (i.e. layer) is composed by one or more tensors.\n",
    "Each child has also a parameter() method that returns each tensor composing the layer.\n",
    "Understanding this structure is important to turn off gradient computation on single layers or sublayers.\n",
    "\"\"\"\n",
    "# load a pretrained model\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "layer_count = 0 \n",
    "layer_max = 4 # until which layer we want the description\n",
    "# loop over the layers of the net\n",
    "for c in model.children():\n",
    "    print(\"*\"*70)\n",
    "    print(f\"CHILDREN {layer_count + 1} IS :\")\n",
    "    print(c)\n",
    "    print('\\n')\n",
    "    print(f\"size of parameters for child {layer_count + 1} are :\")\n",
    "# loop over the tensors composing the layer c\n",
    "    for param in c.parameters():\n",
    "        print(param.shape)\n",
    "        # param.requires_grad = False\n",
    "    print(\"*\"*70)\n",
    "    print('\\n\\n')\n",
    "    layer_count += 1\n",
    "    if layer_count > layer_max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- what is the right terminology for a tensor in a layer?\n",
    "- how is a layer defined?\n",
    "- what is a sub-graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4145, 0.7627, 0.5591,  ..., 0.8221, 0.1405, 0.7418],\n",
       "          [0.8263, 0.3073, 0.5047,  ..., 0.9060, 0.1171, 0.1214],\n",
       "          [0.1013, 0.0171, 0.9138,  ..., 0.7892, 0.8192, 0.1922],\n",
       "          ...,\n",
       "          [0.2389, 0.0933, 0.6571,  ..., 0.9614, 0.1424, 0.6270],\n",
       "          [0.7308, 0.6663, 0.6745,  ..., 0.3075, 0.4274, 0.2564],\n",
       "          [0.1157, 0.7707, 0.7036,  ..., 0.0357, 0.6143, 0.5993]],\n",
       "\n",
       "         [[0.9419, 0.8969, 0.8285,  ..., 0.5001, 0.3291, 0.9373],\n",
       "          [0.2153, 0.5582, 0.2164,  ..., 0.4124, 0.8528, 0.3696],\n",
       "          [0.0113, 0.5700, 0.7718,  ..., 0.2349, 0.8837, 0.2239],\n",
       "          ...,\n",
       "          [0.1147, 0.1277, 0.8975,  ..., 0.6002, 0.9612, 0.9621],\n",
       "          [0.6847, 0.6775, 0.1844,  ..., 0.8157, 0.9225, 0.0109],\n",
       "          [0.6633, 0.3915, 0.7651,  ..., 0.8848, 0.8308, 0.7504]],\n",
       "\n",
       "         [[0.8452, 0.8360, 0.2734,  ..., 0.7499, 0.2969, 0.0283],\n",
       "          [0.6142, 0.8519, 0.5852,  ..., 0.3823, 0.1674, 0.0523],\n",
       "          [0.5229, 0.3796, 0.3087,  ..., 0.7873, 0.1550, 0.1613],\n",
       "          ...,\n",
       "          [0.3596, 0.8222, 0.6888,  ..., 0.2302, 0.2471, 0.5751],\n",
       "          [0.9140, 0.4883, 0.2724,  ..., 0.4199, 0.3399, 0.9323],\n",
       "          [0.7980, 0.9356, 0.8736,  ..., 0.0367, 0.4492, 0.3099]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Autograd is reverse automatic differentiation system.\n",
    "Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations,\n",
    "giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. \n",
    "By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "Internally, autograd represents this graph as a graph of Function objects (really expressions),\n",
    "which can be apply() ed to compute the result of evaluating the graph.\n",
    "When computing the forwards pass, autograd simultaneously performs the requested computations and \n",
    "builds up a graph representing the function that computes the gradient \n",
    "(the .grad_fn attribute of each torch.Tensor is an entry point into this graph). \n",
    "\"\"\"\n",
    "# accessing the graph of Functions for a sublayer in ResNet\n",
    "next(iter(model.parameters()))[0].grad_fn(torch.rand(3,7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd.Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Tensor and Function classes are interconnected. \n",
    "They build up an acyclic graph, that encodes a complete history of computation. \n",
    "Each Tensor has a .grad_fn attribute that references a Function that has created the Tensor \n",
    "(except for Tensors created by the user - their grad_fn is None).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operation that created y = <MulBackward0 object at 0x12a41cf98> is a Function object\n",
      "operation that created z = <PowBackward0 object at 0x12a41cf98> is a Function object\n",
      "operation that created w = <SumBackward0 object at 0x12b0b78d0> is a Function object\n",
      "**********************************************************************\n",
      "\n",
      "compute backward pass starting from w\n",
      "gradient of x from w: tensor([ 8., 16., 24.])\n",
      "\n",
      "By default, gradients are only retained for leaf variables. \n",
      "non-leaf variables’ gradients are not retained to be inspected later. \n",
      "This was done by design, to save memory.\n",
      "\n",
      "gradient of y: None\n",
      "gradient of z: None\n",
      "gradient of w: None\n",
      "\n",
      "compute backward pass starting from z\n",
      "======================================================================\n",
      "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n",
      "======================================================================\n",
      "\n",
      "compute AFTER REDEFINITION backward pass starting from z\n",
      "gradient of x from z: tensor([ 8., 16., 24.])\n",
      "\n",
      "compute AFTER REDEFINITION backward pass starting from y\n",
      "gradient of x from y: tensor([2., 2., 2.])\n",
      "\n",
      "compute AFTER REDEFINITION backward pass starting from w\n",
      "gradient of x from w: tensor([ 8., 16., 24.])\n",
      "\n",
      "**********************************************************************\n",
      "use retain_graph = True\n",
      "**********************************************************************\n",
      "\n",
      "gradient of x from w: tensor([ 8., 16., 24.])\n",
      "gradient of x from z: tensor([16., 32., 48.]),   (.grad accumulates the gradients)\n",
      "gradient of x from y: tensor([18., 34., 50.])\n",
      "\n",
      "**********************************************************************\n",
      "after reinitialization using different \"gradient\" arguments\n",
      "**********************************************************************\n",
      "\n",
      "scalar root, gradient = [1] (default argument)\n",
      "gradient of x from w: tensor([ 8., 16., 24.]) \n",
      "\n",
      "scalar root, gradient = [0]\n",
      "gradient of x from w: tensor([0., 0., 0.]) \n",
      "\n",
      "scalar root, gradient = [1,1,1,1]\n",
      "gradient of x from w: tensor([32., 64., 96.]) \n",
      "\n",
      "scalar root, gradient = [2]\n",
      "gradient of x from w: tensor([16., 32., 48.]) \n",
      " \n",
      " \n",
      "\n",
      "tensorial root, gradient = [1,1,1]\n",
      "gradient of x from z: tensor([ 8., 16., 24.]) \n",
      "\n",
      "tensorial root, gradient = [0,0,0]\n",
      "gradient of x from z: tensor([0., 0., 0.]) \n",
      "\n",
      "tensorial root, gradient = [2,2,2]\n",
      "gradient of x from z: tensor([16., 32., 48.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "print(f'operation that created y = {y.grad_fn} is a Function object' )\n",
    "z = y**2\n",
    "print(f'operation that created z = {z.grad_fn} is a Function object')\n",
    "w = z.sum()\n",
    "print(f'operation that created w = {w.grad_fn} is a Function object', end='\\n' + '*'*70+'\\n\\n')\n",
    "print(f'compute backward pass starting from w')\n",
    "w.backward()\n",
    "print(f'gradient of x from w: {x.grad}')\n",
    "\n",
    "print(\"\"\"\\nBy default, gradients are only retained for leaf variables. \n",
    "non-leaf variables’ gradients are not retained to be inspected later. \n",
    "This was done by design, to save memory.\\n\"\"\")\n",
    "# ref:\n",
    "# https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94\n",
    "print(f'gradient of y: {y.grad}')\n",
    "print(f'gradient of z: {z.grad}')\n",
    "print(f'gradient of w: {w.grad}', end = '\\n\\n')\n",
    "\n",
    "print(f'compute backward pass starting from z')\n",
    "try:\n",
    "    z.backward(torch.FloatTensor([1,1,1]))\n",
    "except Exception as e:\n",
    "    print('='*70)\n",
    "    print(e)\n",
    "    print('='*70)\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "z.backward(torch.FloatTensor([1,1,1]))\n",
    "print()\n",
    "print(f'compute AFTER REDEFINITION backward pass starting from z')\n",
    "print(f'gradient of x from z: {x.grad}')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "y.backward(torch.FloatTensor([1,1,1]))\n",
    "print()\n",
    "print(f'compute AFTER REDEFINITION backward pass starting from y')\n",
    "print(f'gradient of x from y: {x.grad}')\n",
    "print()\n",
    "print(f'compute AFTER REDEFINITION backward pass starting from w')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "w.backward()\n",
    "print(f'gradient of x from w: {x.grad}')\n",
    "\n",
    "print('\\n' + '*'*70)\n",
    "print('use retain_graph = True')\n",
    "print('*'*70+'\\n')\n",
    "\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "ww = 3\n",
    "\n",
    "w.backward(retain_graph=True)\n",
    "print(f'gradient of x from w: {x.grad}')\n",
    "z.backward(torch.FloatTensor([1,1,1]), retain_graph=True)\n",
    "print(f'gradient of x from z: {x.grad},   (.grad accumulates the gradients)'  )\n",
    "y.backward(torch.FloatTensor([1,1,1]))\n",
    "print(f'gradient of x from y: {x.grad}')\n",
    "\n",
    "print('\\n' + '*'*70)\n",
    "print('after reinitialization using different \"gradient\" arguments')\n",
    "print('*'*70+'\\n')\n",
    "\n",
    "print('scalar root, gradient = [1] (default argument)')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "w.backward(torch.FloatTensor([1]))\n",
    "print(f'gradient of x from w: {x.grad} \\n')\n",
    "\n",
    "print('scalar root, gradient = [0]')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "w.backward(torch.FloatTensor([0]))\n",
    "print(f'gradient of x from w: {x.grad} \\n')\n",
    "\n",
    "print('scalar root, gradient = [1,1,1,1]')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "w.backward(torch.FloatTensor([1,1,1,1]))\n",
    "print(f'gradient of x from w: {x.grad} \\n')\n",
    "\n",
    "\n",
    "print('scalar root, gradient = [2]')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "ww = 3\n",
    "w.backward(torch.FloatTensor([2]))\n",
    "print(f'gradient of x from w: {x.grad} \\n \\n \\n')\n",
    "\n",
    "\n",
    "print('tensorial root, gradient = [1,1,1]')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "z.backward(torch.FloatTensor([1,1,1]))\n",
    "print(f'gradient of x from z: {x.grad} \\n')\n",
    "\n",
    "print('tensorial root, gradient = [0,0,0]')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "z.backward(torch.FloatTensor([0,0,0]))\n",
    "print(f'gradient of x from z: {x.grad} \\n')\n",
    "\n",
    "print('tensorial root, gradient = [2,2,2]')\n",
    "x = torch.FloatTensor([1,2,3]) # leaf tensor of Directed Acyclic Graph \n",
    "x.requires_grad = True\n",
    "y = 2.*x \n",
    "z = y**2\n",
    "w = z.sum()\n",
    "z.backward(torch.FloatTensor([2,2,2]))\n",
    "print(f'gradient of x from z: {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending torch.autograd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from the docs:\n",
    "Adding operations to autograd requires implementing a new Function subclass for each operation.\n",
    "Recall that Function s are what autograd uses to compute the results and gradients, and encode the operation history.\n",
    "Every new function requires you to implement 2 methods:\n",
    "1)   forward()\n",
    "2)   backward()\"\"\"\n",
    "\n",
    "# example from programcreek.com\n",
    "\n",
    "def test_function(self):\n",
    "        class MyFunction(Function):\n",
    "\n",
    "            @staticmethod\n",
    "            def forward(ctx, tensor1, scalar, tensor2):\n",
    "# ctx is a context object that can be used to stash information for backward computation\n",
    "                ctx.scalar = scalar\n",
    "                ctx.save_for_backward(tensor1, tensor2)\n",
    "                return tensor1 + scalar * tensor2 + tensor1 * tensor2\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, grad_output):\n",
    "                var1, var2 = ctx.saved_variables\n",
    "                # NOTE: self is the test case here\n",
    "                self.assertIsInstance(var1, Variable)\n",
    "                self.assertIsInstance(var2, Variable)\n",
    "                self.assertIsInstance(grad_output, Variable)\n",
    "                return (grad_output + grad_output * var2, None,\n",
    "                        grad_output * ctx.scalar + grad_output * var1)\n",
    "\n",
    "        x, y = self._function_test(MyFunction)\n",
    "\n",
    "        x_grad_desc = graph_desc(x.grad.grad_fn)\n",
    "        y_grad_desc = graph_desc(y.grad.grad_fn)\n",
    "        self.assertEqual(\n",
    "            x_grad_desc,\n",
    "            'Identity(AddBackward(ExpandBackward(AccumulateGrad()), '\n",
    "            'MulBackward(ExpandBackward(AccumulateGrad()), AccumulateGrad())))')\n",
    "        self.assertEqual(\n",
    "            y_grad_desc,\n",
    "            'Identity(AddBackward(MulConstantBackward(ExpandBackward(AccumulateGrad())), '\n",
    "            'MulBackward(ExpandBackward(AccumulateGrad()), AccumulateGrad())))') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
